Abstract (we'll do this last)

Background

Task + Corpus

This project was initiated as a submission to the ConLL 2013 Shared Task: Grammatical Error Correction \footnote{http://www.comp.nus.edu.sg/~nlp/conll13st.html}. As a result, all training and test data was from a provided corpus, the National University Singapore Corpus of Learner English, or NUCLE, and trained systems were scored with the provided NUS MaxMatch, or M^2 scorer. 

The corpus itself is comprised of ~1400 annotated essays written by english-second-language Singaporean students. The annotations provide an error type and correction, from the below types of errors:

\begin{tabular}{l | c | r}
	\hline
	tag & category \\
	\hline
	Vt & Verb tense \\
	Vm & Verb modal \\
	V0 & Missing verb \\ 
	Vform & Verb form \\
	SVA	& Subject-verb-agreement \\
 	ArtOrDet & Article or Determiner \\
	Nn & Noun number \\
	Npos & Noun possesive \\
	Pform & Pronoun form \\
	Pref & Pronoun reference \\
	Prep & Preposition \\
	Wci & Wrong collocation/idiom \\
	Wa & Acronyms \\
	Wform & Word form \\
	Wtone & Tone \\
	Srun & Runons, comma splice \\
	Smod & Dangling modifier \\
	Spar & Parallelism \\
	Sfrag & Fragment \\
	Ssub & Subordinate clause \\
	WOinc & Incorrect sentence form \\
	WOadv & Adverb/adjective position \\
	Trans & Link word/phrases \\
	Mec	& Punctuation, capitalization, spelling, typos \\
	Rloc & Local redundancy \\
	Cit	& Citation \\
	Others & Other errors \\
	Um & Unclear meaning (cannot be corrected) \\
	\hline
\end{tabular}

For the initial approach to this task, however, we looked to train a system that would identify a set of five errors: SVA, ArtOrDet, Nn, VForm, and Prep. The total corpus amounted to 67372 sentences, of which ~15,000 were noisy (references, urls, etc). Of that, 11288 sentences had annotations with those tags, leaving a fairly small dataset on which to train. After the time of this writing, ConLL has released an additional dataset - the blind test data with the gold references, so we anticipate that this dataset will grow significantly.

Baseline

Our baseline system used Moses to "translate" between a test corpus of original essays and the counterpart essays with all the annotations applied. This "flattened" corpus was split into ~58000 training sentences, ~2000 sentences for tuning, and ~8000 test sentences. 

Moses used Giza++ to align the sentences, with default heuristics and reordering models (grow-diag-final, msd-bidirectional-fe). The tuning used MERT with default features. 

To score the baseline, we use BLEU as well as the M^2 scorer. The M^2 scorer scores precision, accuracy, and F1 against the annotations, rather than the text itself. Though the conference version of M^2 is case-sensitive, we use a case-insensitive version for simplicity - recasing in English is a trivial task. Though both of these scorers support multi-reference evaluation, the corpus itself only has a single-reference gold reference file. Clearly, there are multiple equally fluent machine-generated correction candidates, even within the phrase table generated by the small training corpus, so we think this is an area that needs to be explored in terms of test corpus augmentation.

In reality, the optimality of a "correction" would be gauged by fluency and grammatical cohesion of the final generation, so neither Bleu nor M^2 adequately capture the ideal result - an ideal result would likely be modeled by a combination of the two, with multiple references. 

Issues with Baseline

Corpus Cleaning / Datasets

The first immediate issue with the baseline phrase table was that references and urls generated a large amount of noise in the correction data. Since we used the wordpunct NLTK tokenization scheme, urls in particular were inconsistently treated as multiple words, creating noisy alignments as well. Thus, the first cleaning step was to eliminate references completely, re-adding them after the correction. We accomplished this with a simple reegex substitution. 

Additionally, to experiment on the domain sensitivity of our system, we split the corpus by essay topic. This was particularly necessary because one of the challenges of the shared task was to submit system output for topics both inside and outside the training data. However, since the topics were not available a priori, we used an online version of the Latent Dirichlet Allocation (LDA) algorithm to generate a topic model, and then a K-Means clustering implementation to split the documents by those topics. We were then able to generate a "held out" dataset with 21572 training sentences (no references) with all but one topic, and 8695 test sentences (no references) with the remaining topic. This experiment showed that the NUCLE essay topics were largely similar in content, and that domain sensitivity is still a concern that ought to be tested by a heldout test set with a topic further removed from the training corpus.

To experiment with the impact of zero-annotation sentences in the training corpus, we also generated datasets without correct sentences. This left a dataset of 11288 sentences, of which we used 10000 to train and 288 to tune. 

To run stemming experiments, we used the stemmers bundled with NLTK. We stemmed and lemmatized the two corpora mentioned above with the Lancaster and Snowball stemmers and the WordNet lemmatizer to experiment with various degrees of stemming aggressiveness.

We used a Moses (SRILM) ngram language model trained on the test and dev datasets. Given the topic closeness between the test and train sets and the relatively small size of the test set, there was not a significant OOV problem, and experiments with big language models trained on the Brown Corpus did not yield any benefits to either of the scoring metrics. However, future work wil

Approach

Results

Observations

Future Work

References

Daniel Dahlmeier and Hwee Tou Ng. 2012. Better Evaluation for Grammatical Error Correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies